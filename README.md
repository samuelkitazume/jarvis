## Arquitetura: Assistente Virtual Local (Estilo Jarvis)

### 1. Vis√£o Geral
Um sistema 100% local e modular, com foco em:
- Modelo de linguagem local (LLM)
- Personalidade customizada (prompt ou LoRA)
- Execu√ß√£o de APIs locais (Home Assistant, etc.)
- Mem√≥ria contextual e racioc√≠nio (via agente ou RAG)
- Integra√ß√£o externa com STT e TTS em outros dispositivos
- Empacotado com Docker Compose para facilitar o deploy

---

### 2. Componentes da Arquitetura

#### üß† LLM (Modelo de Linguagem)
- Ex: Mistral 7B, Phi-2, OpenHermes, Nous-Hermes
- Ferramenta: [Ollama](https://ollama.com), LM Studio ou text-generation-webui
- Entrada: Texto do usu√°rio (via STT externo ou texto direto)
- Sa√≠da: Resposta textual processada
- Container: `ollama`
- Observa√ß√£o: O Ollama exp√µe uma API REST em `11434` que pode ser acessada por outros containers ou dispositivos. O LangChain se conecta a ela usando `base_url="http://ollama:11434"` (ou `ollama` como nome de host dentro da rede Docker Compose).

#### üåü Personalidade / Estilo de Fala
- Prompt de sistema fixo com instru√ß√µes de estilo
- (Opcional) LoRA para trejeitos e personalidade
- Configur√°vel via arquivos externos montados como volume

#### üßπ Racioc√≠nio + Execu√ß√£o de APIs
- Framework: LangChain, PrivateGPT ou script customizado
- Entrada: Pedido do usu√°rio
- A√ß√£o: Interpreta e chama APIs (Home Assistant, etc.)
- Container: `jarvis-backend`

#### ü§î Mem√≥ria e Contexto (RAG)
- Vetor store: Chroma, FAISS, Weaviate
- Uso: Indexa documentos, registros de intera√ß√£o e prefer√™ncias
- Container: `vector-db`
- Observa√ß√£o: O RAG √© implementado via LangChain e utiliza o Ollama j√° existente. N√£o √© iniciado um novo LLM; o LangChain apenas consome a API do Ollama. O processo envolve: carregar documentos, gerar embeddings (ex: MiniLM), armazenar no Chroma e, ao consultar, enviar os trechos recuperados junto com o prompt para o LLM gerar a resposta.

#### üõ† Integra√ß√£o com Smart Home
- Home Assistant via REST API, MQTT ou WebSocket
- Intera√ß√£o com sensores, dispositivos Tuya/Zigbee/Wi-Fi
- Container externo: `home-assistant`

#### üåé Integra√ß√£o Externa: STT e TTS
- Dispositivo externo (outro PC, Raspberry Pi, ou container separado)
- STT captura voz e envia texto para API da LLM
- LLM responde com texto
- TTS converte resposta textual em √°udio e fala com o usu√°rio

---

### 3. Fluxo de Dados Modular

**[Dispositivo Externo]**
1. STT (Speech-to-text)
2. Envia prompt para API da LLM (Kali Linux)

**[Servidor Kali Linux]**
3. LLM interpreta comando + aplica personalidade
4. Agente decide se chama API ou responde direto
5. (Opcional) Consulta ao vetor store (RAG)
6. Resposta textual √© devolvida pela API

**[Dispositivo Externo]**
7. Recebe resposta textual
8. TTS gera voz e reproduz

---

### 4. Requisitos de Hardware

#### ‚ö° Setup Modesto (sem RAG, modelo menor)
- CPU com 4+ threads
- 8 GB RAM
- SSD 256 GB+
- (Opcional) GPU integrada ou modesta (Intel UHD, AMD Vega)
- Modelo: Phi-2, Mistral 7B quantizado (Q4)

#### üî• Setup Intermedi√°rio (com RAG)
- CPU 6+ cores
- 16 GB RAM
- SSD 512 GB+
- GPU com 8-12 GB VRAM (ex: RTX 3060, 4060)
- Modelo: Mistral 7B, Nous Hermes, OpenHermes
- Vector DB para contexto

#### üåå Setup Avan√ßado (conhecimento, alta fluidez)
- CPU 8+ cores
- 32 GB RAM
- SSD 1 TB NVMe
- GPU com 24+ GB VRAM (ex: RTX 4090)
- Modelo: Mixtral 8x7B, MythoMax 13B, LLaMA 2 13B+
- RAG com banco vetorial completo

---

### 5. Justificativa Arquitetural: Separar STT/TTS do LLM
- A separa√ß√£o de responsabilidades permite que o servidor Kali Linux atue como "c√©rebro" central (LLM + RAG), enquanto outro dispositivo lida com voz (STT/TTS)
- Em redes locais, a lat√™ncia √© baix√≠ssima e praticamente impercept√≠vel (<5ms)
- O TTS pode ser rodado em um desktop com GPU (ou mesmo CPU) sem sobrecarregar o backend
- TTSs mais pesados como Bark ou Tortoise podem ser processados offline e reproduzidos via rede
- A arquitetura facilita atualiza√ß√µes, manuten√ß√£o e expans√£o futura para v√°rios dispositivos

---

### 6. Expans√µes Futuras
- Controle por gesto ou c√¢mera
- Fine-tuning com dados pessoais (LoRA customizado)
- Multiusu√°rio com identifica√ß√£o de voz
- Dashboard web com logs + comandos
- Interface visual tipo "Jarvis HUD"
- Gerenciamento dos containers via WebUI
- Distribui√ß√£o modular com v√°rios dispositivos em rede


