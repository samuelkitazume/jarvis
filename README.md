## Arquitetura: Assistente Virtual Local (Estilo Jarvis)

### 1. Vis√£o Geral
Um sistema 100% local e modular, com foco em:
- Modelo de linguagem local (LLM)
- Personalidade customizada (prompt ou LoRA)
- Execu√ß√£o de APIs locais (Home Assistant, etc.)
- Mem√≥ria contextual e racioc√≠nio (via agente ou RAG)
- Integra√ß√£o externa com STT e TTS em outros dispositivos
- Empacotado com Docker Compose para facilitar o deploy

---

### 2. Componentes da Arquitetura

#### üß† LLM (Modelo de Linguagem)
- Ex: Mistral 7B, Phi-2, OpenHermes, Nous-Hermes
- Ferramenta: [Ollama](https://ollama.com), LM Studio ou text-generation-webui
- Entrada: Texto do usu√°rio (via STT externo ou texto direto)
- Sa√≠da: Resposta textual processada
- Container: `ollama`
- Observa√ß√£o: O Ollama exp√µe uma API REST em `11434` que pode ser acessada por outros containers ou dispositivos. O LangChain se conecta a ela usando `base_url="http://ollama:11434"` (ou `ollama` como nome de host dentro da rede Docker Compose).

#### üåü Personalidade / Estilo de Fala
- Prompt de sistema fixo com instru√ß√µes de estilo
- (Opcional) LoRA para trejeitos e personalidade
- Configur√°vel via arquivos externos montados como volume

#### ü§πÔ∏è Racioc√≠nio + Execu√ß√£o de APIs
- Framework: LangChain, PrivateGPT ou script customizado
- Entrada: Pedido do usu√°rio
- A√ß√£o: Interpreta e chama APIs (Home Assistant, etc.)
- Container: `jarvis-backend`
- Integra√ß√£o via OpenAPI Schema: utilizando `langchain.tools.openapi`, √© poss√≠vel importar um schema OpenAPI (ex: do Home Assistant) e permitir que o modelo:
  - Analise os endpoints dispon√≠veis
  - Decida qual usar com base na inten√ß√£o do usu√°rio
  - Monte automaticamente a requisi√ß√£o HTTP
  - Interprete a resposta da API e gere uma resposta natural
- Observa√ß√£o: Essa funcionalidade atua como uma extens√£o dos agentes, semelhante a um RAG, mas com foco em a√ß√µes din√¢micas e integra√ß√µes externas ao inv√©s de contexto documental.

- **Comparativo RAG vs OpenAPI Agent**:

| Caso de uso                     | Melhor com...        | Justificativa                                      |
|---------------------------------|-----------------------|----------------------------------------------------|
| ‚ÄúComo fa√ßo pra resetar a Alexa?‚Äù| RAG                   | Informa√ß√£o est√°vel, escrita 1x, √∫til sempre        |
| ‚ÄúQuantas fraldas hoje de manh√£?‚Äù| OpenAPI Agent         | Dados variam, dependem de filtros e tempo          |
| ‚ÄúQuais sensores est√£o ativos?‚Äù  | OpenAPI Agent         | Tempo real, consulta din√¢mica                      |
| ‚ÄúQual a rotina ‚ÄòBoa noite‚Äô?‚Äù    | RAG                   | Texto definido e raramente modificado              |
| ‚ÄúQual foi o √∫ltimo banho?‚Äù      | OpenAPI Agent         | Dados estruturados, tempo sens√≠vel                 |

---

#### ü§î Mem√≥ria e Contexto (RAG)
- Vetor store: Chroma, FAISS, Weaviate
- Uso: Indexa documentos, registros de intera√ß√£o e prefer√™ncias que sejam essencialmente **est√°ticos ou com pouca varia√ß√£o**, como FAQs, manuais, instru√ß√µes da casa, listas de comandos, regras e protocolos.
- Container: `vector-db`
- Observa√ß√£o: O RAG √© implementado via LangChain e utiliza o Ollama j√° existente. N√£o √© iniciado um novo LLM; o LangChain apenas consome a API do Ollama. O processo envolve: carregar documentos, gerar embeddings (ex: MiniLM), armazenar no Chroma e, ao consultar, enviar os trechos recuperados junto com o prompt para o LLM gerar a resposta.
- Limita√ß√£o: Embora seja eficiente para contexto est√°tico, o RAG n√£o lida bem com **dados altamente din√¢micos** (como logs atualizados, sensores, e-mails, atividades de beb√™, etc.), onde o uso de **agentes com integra√ß√£o via OpenAPI** √© mais apropriado e flex√≠vel.
- Recomenda√ß√£o: Use RAG para dados de refer√™ncia duradouros (manuais, comandos, regras fixas), e agentes para dados vivos ou em constante muta√ß√£o.

---

### 3. Fluxo de Dados Modular

**[Dispositivo Externo]**
1. STT (Speech-to-text)
2. Envia prompt para API da LLM (Kali Linux)

**[Servidor Kali Linux]**
3. LLM interpreta comando + aplica personalidade
4. Agente decide se chama API (via OpenAPI) ou responde direto
5. (Opcional) Consulta ao vetor store (RAG)
6. Resposta textual √© devolvida pela API

**[Dispositivo Externo]**
7. Recebe resposta textual
8. TTS gera voz e reproduz

---

### 4. Requisitos de Hardware

#### ‚ö° Setup Modesto (sem RAG, modelo menor)
- CPU com 4+ threads
- 8 GB RAM
- SSD 256 GB+
- (Opcional) GPU integrada ou modesta (Intel UHD, AMD Vega)
- Modelo: Phi-2, Mistral 7B quantizado (Q4)

#### üî• Setup Intermedi√°rio (com RAG)
- CPU 6+ cores
- 16 GB RAM
- SSD 512 GB+
- GPU com 8-12 GB VRAM (ex: RTX 3060, 4060)
- Modelo: Mistral 7B, Nous Hermes, OpenHermes
- Vector DB para contexto

#### üåå Setup Avan√ßado (conhecimento, alta fluidez)
- CPU 8+ cores
- 32 GB RAM
- SSD 1 TB NVMe
- GPU com 24+ GB VRAM (ex: RTX 4090)
- Modelo: Mixtral 8x7B, MythoMax 13B, LLaMA 2 13B+
- RAG com banco vetorial completo

---

### 5. Justificativa Arquitetural: Separar STT/TTS do LLM
- A separa√ß√£o de responsabilidades permite que o servidor Kali Linux atue como "c√©rebro" central (LLM + RAG), enquanto outro dispositivo lida com voz (STT/TTS)
- Em redes locais, a lat√™ncia √© baix√≠ssima e praticamente impercept√≠vel (<5ms)
- O TTS pode ser rodado em um desktop com GPU (ou mesmo CPU) sem sobrecarregar o backend
- TTSs mais pesados como Bark ou Tortoise podem ser processados offline e reproduzidos via rede
- A arquitetura facilita atualiza√ß√µes, manuten√ß√£o e expans√£o futura para v√°rios dispositivos

---

### 6. Expans√µes Futuras
- Controle por gesto ou c√¢mera
- Fine-tuning com dados pessoais (LoRA customizado)
- Multiusu√°rio com identifica√ß√£o de voz
- Dashboard web com logs + comandos
- Interface visual tipo "Jarvis HUD"
- Gerenciamento dos containers via WebUI
- Distribui√ß√£o modular com v√°rios dispositivos em rede


### Dicas

#### Curl para jarvis-backend
```bash
curl -X POST http://localhost:5000/ask \
  -H "Content-Type: application/json" \
  -d '{"question": "Hey, Jarvis!"}'
```

#### Curl para Ollama direto
```bash
curl http://localhost:11434/api/generate \
  -d '{
    "model": "phi",
    "prompt": "What is the capital of Canada?",
    "stream": false
  }' \
  -H "Content-Type: application/json"
```

